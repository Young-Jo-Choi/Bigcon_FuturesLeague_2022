{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "# import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test split\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# models\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import plot_importance\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# imbalanced learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "with gzip.open('../preprocessed/final_data.pickle','rb') as f:\n",
    "    final = pickle.load(f)\n",
    "\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완전 중복되는 열인지 확인\n",
    "# final['desired_amount_x'].equals(final['desired_amount_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 column 발견해서 여럿 drop\n",
    "# final = final.drop(['desired_amount_x', 'income_type', 'employment_type'], axis=1)\n",
    "final = final.drop(['desired_amount_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_temp=pd.concat([final.iloc[:,0],final.iloc[:,3:]],axis=1)\n",
    "len(final_temp.columns)\n",
    "final=final_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.rename(columns={'desired_amount_y': 'desired_amount'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in final.columns if \"_x\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6833617, 39)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final.drop(['is_applied', 'application_id', 'user_id'], axis = 1)\n",
    "y = final['is_applied']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature별 값의 범위 스케일 해주기\n",
    "\n",
    "# standard scaler (z-score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X만 본인이 가지고 있는 변수로 넣어주기 (X는 label 없이 only feature만!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler 피클 파일로 저장\n",
    "with open('../preprocessed/scaler_final.pickle','wb') as f:\n",
    "    pickle.dump(scaler, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state=777, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 4535212), (1.0, 248319)]\n"
     ]
    }
   ],
   "source": [
    "# train set 클래스\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio :  1\n",
      "after under sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    248319\n",
       "1.0    248319\n",
       "Name: is_applied, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after smote\n",
      "smote 후 데이터 class별 개수 [(0.0, 248319), (1.0, 248319)]\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.89   1943664\n",
      "         1.0       0.20      0.88      0.32    106422\n",
      "\n",
      "    accuracy                           0.81   2050086\n",
      "   macro avg       0.59      0.84      0.60   2050086\n",
      "weighted avg       0.95      0.81      0.86   2050086\n",
      "\n",
      "LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.89   1943664\n",
      "         1.0       0.19      0.88      0.32    106422\n",
      "\n",
      "    accuracy                           0.80   2050086\n",
      "   macro avg       0.59      0.84      0.60   2050086\n",
      "weighted avg       0.95      0.80      0.86   2050086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryu/.conda/envs/ryuvenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.76      0.86   1943664\n",
      "         1.0       0.16      0.82      0.26    106422\n",
      "\n",
      "    accuracy                           0.76   2050086\n",
      "   macro avg       0.57      0.79      0.56   2050086\n",
      "weighted avg       0.94      0.76      0.83   2050086\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.89   1943664\n",
      "         1.0       0.20      0.88      0.32    106422\n",
      "\n",
      "    accuracy                           0.81   2050086\n",
      "   macro avg       0.59      0.84      0.60   2050086\n",
      "weighted avg       0.95      0.81      0.86   2050086\n",
      "\n",
      "ratio :  0.5\n",
      "after under sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    496638\n",
       "1.0    248319\n",
       "Name: is_applied, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after smote\n",
      "smote 후 데이터 class별 개수 [(0.0, 496638), (1.0, 496638)]\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.86      0.92   1943664\n",
      "         1.0       0.24      0.79      0.37    106422\n",
      "\n",
      "    accuracy                           0.86   2050086\n",
      "   macro avg       0.61      0.82      0.64   2050086\n",
      "weighted avg       0.95      0.86      0.89   2050086\n",
      "\n",
      "LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.86      0.92   1943664\n",
      "         1.0       0.24      0.79      0.37    106422\n",
      "\n",
      "    accuracy                           0.86   2050086\n",
      "   macro avg       0.61      0.83      0.64   2050086\n",
      "weighted avg       0.95      0.86      0.89   2050086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryu/.conda/envs/ryuvenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.76      0.86   1943664\n",
      "         1.0       0.16      0.82      0.26    106422\n",
      "\n",
      "    accuracy                           0.76   2050086\n",
      "   macro avg       0.57      0.79      0.56   2050086\n",
      "weighted avg       0.94      0.76      0.83   2050086\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.86      0.92   1943664\n",
      "         1.0       0.23      0.81      0.36    106422\n",
      "\n",
      "    accuracy                           0.85   2050086\n",
      "   macro avg       0.61      0.83      0.64   2050086\n",
      "weighted avg       0.95      0.85      0.89   2050086\n",
      "\n",
      "ratio :  0.33\n",
      "after under sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    752481\n",
       "1.0    248319\n",
       "Name: is_applied, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after smote\n",
      "smote 후 데이터 class별 개수 [(0.0, 752481), (1.0, 752481)]\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.90      0.94   1943664\n",
      "         1.0       0.27      0.70      0.40    106422\n",
      "\n",
      "    accuracy                           0.89   2050086\n",
      "   macro avg       0.63      0.80      0.67   2050086\n",
      "weighted avg       0.95      0.89      0.91   2050086\n",
      "\n",
      "LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.90      0.94   1943664\n",
      "         1.0       0.28      0.70      0.40    106422\n",
      "\n",
      "    accuracy                           0.89   2050086\n",
      "   macro avg       0.63      0.80      0.67   2050086\n",
      "weighted avg       0.95      0.89      0.91   2050086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryu/.conda/envs/ryuvenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.76      0.86   1943664\n",
      "         1.0       0.16      0.81      0.26    106422\n",
      "\n",
      "    accuracy                           0.76   2050086\n",
      "   macro avg       0.57      0.79      0.56   2050086\n",
      "weighted avg       0.94      0.76      0.83   2050086\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.89      0.93   1943664\n",
      "         1.0       0.27      0.75      0.39    106422\n",
      "\n",
      "    accuracy                           0.88   2050086\n",
      "   macro avg       0.63      0.82      0.66   2050086\n",
      "weighted avg       0.95      0.88      0.91   2050086\n",
      "\n",
      "ratio :  0.2\n",
      "after under sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    1241595\n",
       "1.0     248319\n",
       "Name: is_applied, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after smote\n",
      "smote 후 데이터 class별 개수 [(0.0, 1241595), (1.0, 1241595)]\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.93      0.95   1943664\n",
      "         1.0       0.32      0.58      0.42    106422\n",
      "\n",
      "    accuracy                           0.92   2050086\n",
      "   macro avg       0.65      0.76      0.69   2050086\n",
      "weighted avg       0.94      0.92      0.93   2050086\n",
      "\n",
      "LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96   1943664\n",
      "         1.0       0.33      0.58      0.42    106422\n",
      "\n",
      "    accuracy                           0.92   2050086\n",
      "   macro avg       0.65      0.76      0.69   2050086\n",
      "weighted avg       0.94      0.92      0.93   2050086\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryu/.conda/envs/ryuvenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.76      0.86   1943664\n",
      "         1.0       0.16      0.81      0.26    106422\n",
      "\n",
      "    accuracy                           0.76   2050086\n",
      "   macro avg       0.57      0.79      0.56   2050086\n",
      "weighted avg       0.94      0.76      0.83   2050086\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.92      0.95   1943664\n",
      "         1.0       0.31      0.65      0.42    106422\n",
      "\n",
      "    accuracy                           0.91   2050086\n",
      "   macro avg       0.65      0.79      0.69   2050086\n",
      "weighted avg       0.95      0.91      0.92   2050086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "for ratio in [1,0.5,0.33,0.2]:\n",
    "    print('ratio : ',ratio)\n",
    "    # ratio = (number of samples in the minority class) / (number of samples in the majority class)\n",
    "    random_under = RandomUnderSampler(sampling_strategy=ratio, random_state=777) # 1:1 (X), 1:2 (0.5), 1:3 (0.33), 1:5 (0.2)\n",
    "    X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('after under sampling')\n",
    "    display(pd.Series(y_under).value_counts())\n",
    "    print('\\nafter smote')\n",
    "    # SMOTE\n",
    "\n",
    "    \n",
    "    print('smote 후 데이터 class별 개수', sorted(Counter(y_resampled).items()))\n",
    "\n",
    "    # no tuning, base model\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(random_state=777, tree_method='gpu_hist', gpu_id=0)\n",
    "    xgb_model.fit(X_resampled, y_resampled)\n",
    "    print('XGBoost')\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    # LGBM\n",
    "    lgbm = LGBMClassifier(random_state=777, n_jobs=-1, n_estimators=200, objective='binary', is_unbalance=True) # device='gpu', \n",
    "    lgbm.fit(X_resampled, y_resampled)\n",
    "    print('LightGBM')\n",
    "    y_pred_lgbm = lgbm.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_lgbm)) # SMOTE까지 한 f1=0.4,\n",
    "\n",
    "    # Logistic\n",
    "    lr_base = LogisticRegression(random_state=777)\n",
    "    lr_base.fit(X_resampled, y_resampled)\n",
    "    print('Logistic')\n",
    "    y_pred_lr = lr_base.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "    # Random Forest\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 100, random_state=777)\n",
    "    rf_clf.fit(X_resampled, y_resampled)\n",
    "    print('Random Forest')\n",
    "    y_preds_rf = rf_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_preds_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ratio 변경해서 추가적으로 더 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio :  0.125\n",
      "after under sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    1986552\n",
       "1.0     248319\n",
       "Name: is_applied, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after smote\n",
      "smote 후 데이터 class별 개수 [(0.0, 1986552), (1.0, 1986552)]\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96   1943664\n",
      "         1.0       0.36      0.54      0.43    106422\n",
      "\n",
      "    accuracy                           0.93   2050086\n",
      "   macro avg       0.67      0.74      0.69   2050086\n",
      "weighted avg       0.94      0.93      0.93   2050086\n",
      "\n",
      "ratio :  0.1\n",
      "after under sampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    2483190\n",
       "1.0     248319\n",
       "Name: is_applied, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after smote\n",
      "smote 후 데이터 class별 개수 [(0.0, 2483190), (1.0, 2483190)]\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.96   1943664\n",
      "         1.0       0.38      0.48      0.43    106422\n",
      "\n",
      "    accuracy                           0.93   2050086\n",
      "   macro avg       0.68      0.72      0.69   2050086\n",
      "weighted avg       0.94      0.93      0.94   2050086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "for ratio in [0.125, 0.1]:\n",
    "    print('ratio : ',ratio)\n",
    "    # ratio = (number of samples in the minority class) / (number of samples in the majority class)\n",
    "    random_under = RandomUnderSampler(sampling_strategy=ratio, random_state=777) # 1:1 (X), 1:2 (0.5), 1:3 (0.33), 1:5 (0.2)\n",
    "    X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('after under sampling')\n",
    "    display(pd.Series(y_under).value_counts())\n",
    "    print('\\nafter smote')\n",
    "    # SMOTE\n",
    "\n",
    "    smote = SMOTE(random_state=777) # SMOTE의 하이퍼파라미터는 default로 사용\n",
    "    X_resampled, y_resampled=smote.fit_resample(X_under, y_under)\n",
    "    print('smote 후 데이터 class별 개수', sorted(Counter(y_resampled).items()))\n",
    "\n",
    "    # no tuning, base model\n",
    "    # Random Forest\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 100, random_state=777)\n",
    "    rf_clf.fit(X_resampled, y_resampled)\n",
    "    print('Random Forest')\n",
    "    y_preds_rf = rf_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_preds_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models without SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# undersampling 비율 별로 모델별 성능이 어떻게 되는지 확인\n",
    "for ratio in [1,0.5,0.2]:\n",
    "    print('ratio : ',ratio)\n",
    "    # ratio = (number of samples in the minority class) / (number of samples in the majority class)\n",
    "    random_under = RandomUnderSampler(sampling_strategy=ratio, random_state=777) # 1:1 (X), 1:2 (0.5), 1:3 (0.33), 1:5 (0.2)\n",
    "    X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('after under sampling')\n",
    "    display(pd.Series(y_under).value_counts())\n",
    "    \n",
    "    # no tuning, base model\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(random_state=777, tree_method='gpu_hist', gpu_id=0)\n",
    "    xgb_model.fit(X_under, y_under)\n",
    "    print('XGBoost')\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    # LGBM\n",
    "    lgbm = LGBMClassifier(random_state=777, n_jobs=-1, n_estimators=200, objective='binary', is_unbalance=True) # device='gpu', \n",
    "    lgbm.fit(X_under, y_under)\n",
    "    print('LightGBM')\n",
    "    y_pred_lgbm = lgbm.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_lgbm)) # SMOTE까지 한 f1=0.4,\n",
    "\n",
    "    # Logistic\n",
    "    lr_base = LogisticRegression(random_state=777)\n",
    "    lr_base.fit(X_under, y_under)\n",
    "    print('Logistic')\n",
    "    y_pred_lr = lr_base.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "    # Random Forest\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 100, random_state=777)\n",
    "    rf_clf.fit(X_under, y_under)\n",
    "    print('Random Forest')\n",
    "    y_preds_rf = rf_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_preds_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest without SMOTE 비율 달리해서 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "for ratio in [0.17, 0.143, 0.125, 0.1]:\n",
    "    print('ratio : ',ratio)\n",
    "    # ratio = (number of samples in the minority class) / (number of samples in the majority class)\n",
    "    random_under = RandomUnderSampler(sampling_strategy=ratio, random_state=777) # 1:1 (X), 1:2 (0.5), 1:3 (0.33), 1:5 (0.2)\n",
    "    X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('after under sampling')\n",
    "    display(pd.Series(y_under).value_counts())\n",
    "    \n",
    "    # no tuning, base model\n",
    "    # Random Forest\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 100, random_state=777)\n",
    "    rf_clf.fit(X_under, y_under)\n",
    "    print('Random Forest')\n",
    "    y_preds_rf = rf_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_preds_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning (\"without\" SMOTE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_under = RandomUnderSampler(sampling_strategy=0.2, random_state=777)\n",
    "X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "estimators = [100, 150, 200, 250, 300, 350]\n",
    "# min_child_weight = [] # default 1\n",
    "max_depths = [20, 30] # default 6\n",
    "subsamples = [0.7, 1.0] # default 1\n",
    "gammas = [0, 5, 10]\n",
    "\n",
    "from itertools import product as prod\n",
    "\n",
    "for n_estimator, max_depth, subsample, gamma in prod(\n",
    "    estimators, max_depths, subsamples, gammas):\n",
    "    \n",
    "    xgbc = xgb.XGBClassifier(random_state = 777, tree_method='gpu_hist', gpu_id=0,\n",
    "            n_estimators = n_estimator,\n",
    "            max_depth = max_depth,\n",
    "            subsample = subsample,\n",
    "            gamma = gamma,\n",
    "            )\n",
    "    xgbc.fit(X_under, y_under)\n",
    "\n",
    "    y_train_pred_xgb = xgbc.predict(X_train)\n",
    "    y_under_pred_xgb = xgbc.predict(X_under)\n",
    "    y_test_pred_xgb = xgbc.predict(X_test)\n",
    "\n",
    "    print(f'n_estimators : {n_estimator}, max_depth : {max_depth}, subsample : {subsample}, gamma : {gamma}')\n",
    "    print(f'f1 score with train set: {f1_score(y_train, y_train_pred_xgb)}')\n",
    "    print(f'f1 score with test set: {f1_score(y_test, y_test_pred_xgb)}')\n",
    "    print(f'f1 score with under: {f1_score(y_under, y_under_pred_xgb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tune = xgb.XGBClassifier(random_state = 777, tree_method='gpu_hist', gpu_id=0,\n",
    "            n_estimators=100, max_depth=20, subsample=1.0, gamma=0)\n",
    "xgb_tune.fit(X_under, y_under)\n",
    "proba = xgb_tune.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def is_positive(x, thres):\n",
    "    if x>thres:\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 0\n",
    "    return y\n",
    "\n",
    "def get_best_combination(proba, real):\n",
    "    best_score = 0\n",
    "    best_thres = 0\n",
    "    for thres in np.arange(0.1,0.9, 0.01):\n",
    "        score = f1_score(is_positive(proba, thres), real)\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_thres = thres\n",
    "    return best_score, best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4452837447425564"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1_score(is_positive(proba[:,1], 0.5), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.445409073241964, 0.47999999999999976)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_combination(proba[:,1], y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE 사용하지 않는 random forest tuning\n",
    "random_under = RandomUnderSampler(sampling_strategy=0.2, random_state=777)\n",
    "X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "n_estimators = [150,250]\n",
    "max_depths = [20, 30]\n",
    "min_samples_splits = [8,16]\n",
    "min_samples_leafs = [8,18]\n",
    "\n",
    "for n_estimator, max_depth, min_samples_split, min_samples_leaf in prod(\n",
    "    n_estimators, max_depths, min_samples_splits, min_samples_leafs):\n",
    "    \n",
    "    rf_clf = RandomForestClassifier(random_state = 777,\n",
    "            n_estimators = n_estimator,\n",
    "            max_depth = max_depth,\n",
    "            min_samples_split = min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf)\n",
    "    rf_clf.fit(X_under, y_under)\n",
    "\n",
    "    y_train_pred = rf_clf.predict(X_train)\n",
    "    y_under_pred = rf_clf.predict(X_under)\n",
    "    y_test_pred = rf_clf.predict(X_test)\n",
    "    print(f'n_estimators : {n_estimator}, max_depth : {max_depth}, min_samples_split : {min_samples_split}, min_samples_leaf : {min_samples_leaf}')\n",
    "    print(f'f1 score with train set: {f1_score(y_train, y_train_pred)}')\n",
    "    print(f'f1 score with test set: {f1_score(y_test, y_test_pred)}')\n",
    "    print(f'f1 score with under: {f1_score(y_under, y_under_pred)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solver : lbfgs, C : 100\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : lbfgs, C : 10\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : lbfgs, C : 1.0\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : lbfgs, C : 0.1\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : lbfgs, C : 0.01\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : sag, C : 100\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : sag, C : 10\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : sag, C : 1.0\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : sag, C : 0.1\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : sag, C : 0.01\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : saga, C : 100\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : saga, C : 10\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : saga, C : 1.0\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : saga, C : 0.1\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n",
      "solver : saga, C : 0.01\n",
      "f1 score with train set: 0.731668701020715\n",
      "f1 score with test set: 0.4362482127119604\n",
      "f1 score with under: 0.9993737880987522\n"
     ]
    }
   ],
   "source": [
    "random_under = RandomUnderSampler(sampling_strategy=0.2, random_state=777)\n",
    "X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "solvers = ['lbfgs', 'sag', 'saga']\n",
    "C_list = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "\n",
    "for solver, c_value in prod(solvers, C_list):\n",
    "    \n",
    "    logistic = LogisticRegression(random_state = 777,\n",
    "            solver = solver,\n",
    "            penalty='l2',\n",
    "            C = c_value, n_jobs=-1)\n",
    "    xgbc.fit(X_under, y_under)\n",
    "\n",
    "    y_train_pred_log = xgbc.predict(X_train)\n",
    "    y_under_pred_log = xgbc.predict(X_under)\n",
    "    y_test_pred_log = xgbc.predict(X_test)\n",
    "\n",
    "    print(f'solver : {solver}, C : {c_value}')\n",
    "    print(f'f1 score with train set: {f1_score(y_train, y_train_pred_log)}')\n",
    "    print(f'f1 score with test set: {f1_score(y_test, y_test_pred_log)}')\n",
    "    print(f'f1 score with under: {f1_score(y_under, y_under_pred_log)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 튜닝한 파라미터들로 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_under = RandomUnderSampler(sampling_strategy=0.2, random_state=777)\n",
    "X_under, y_under = random_under.fit_resample(X_train, y_train)\n",
    "\n",
    "smote = SMOTE(random_state=777) # SMOTE의 하이퍼파라미터는 default로 사용\n",
    "X_resampled, y_resampled=smote.fit_resample(X_under, y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost done\n",
      "lgbm done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryu/.conda/envs/ryuvenv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic done\n",
      "random forest done\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb_tuned = xgb.XGBClassifier(random_state=777, n_estimators=100, \n",
    "                            max_depth=20, subsample=1.0, gamma=0, \n",
    "                            tree_method='gpu_hist', gpu_id=0)\n",
    "xgb_tuned.fit(X_under, y_under)\n",
    "print('xgboost done')\n",
    "\n",
    "# LGBM\n",
    "lgbm_tuned = LGBMClassifier(num_leaves=70, max_depth=25, min_child_samples=100, \n",
    "                        random_state=777, n_jobs=-1, n_estimators=200, objective='binary', \n",
    "                        is_unbalance=True)\n",
    "lgbm_tuned.fit(X_resampled, y_resampled)\n",
    "print('lgbm done')\n",
    "\n",
    "# Logistic\n",
    "lr_tuned = LogisticRegression(random_state=777, penalty='l2', C=1, n_jobs=-1)\n",
    "lr_tuned.fit(X_under, y_under)\n",
    "print('logistic done')\n",
    "\n",
    "# Random Forest\n",
    "rf_tuned = RandomForestClassifier(n_estimators=100, random_state=777)\n",
    "rf_tuned.fit(X_under, y_under)\n",
    "print('random forest done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 앙상블: 경우의 수 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def is_positive(x, thres):\n",
    "    if x>thres:\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 0\n",
    "    return y\n",
    "\n",
    "def get_best_combination(proba, real):\n",
    "    best_score = 0\n",
    "    best_thres = 0\n",
    "    for thres in np.arange(0.1,0.9, 0.01):\n",
    "        score = f1_score(is_positive(proba, thres), real)\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_thres = thres\n",
    "    return best_score, best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 1 0.44022071433897547 0.47999999999999976\n",
      "0 0 1 0 0.3397625968205511 0.3899999999999999\n",
      "0 0 1 1 0.4161516184610873 0.4299999999999998\n",
      "0 1 0 0 0.4280672831714793 0.5199999999999998\n",
      "0 1 0 1 0.4419547912742298 0.4999999999999998\n",
      "0 1 1 0 0.41012390284848815 0.44999999999999984\n",
      "0 1 1 1 0.4313505916659464 0.45999999999999985\n",
      "1 0 0 0 0.443500899228404 0.47999999999999976\n",
      "1 0 0 1 0.4497652686534535 0.47999999999999976\n",
      "1 0 1 0 0.44207843734559754 0.43999999999999984\n",
      "1 0 1 1 0.44746560002396946 0.44999999999999984\n",
      "1 1 0 0 0.4501996568913771 0.48999999999999977\n",
      "1 1 0 1 0.4520239383433076 0.47999999999999976\n",
      "1 1 1 0 0.4460009627691928 0.45999999999999985\n",
      "1 1 1 1 0.44899313842482097 0.45999999999999985\n"
     ]
    }
   ],
   "source": [
    "from itertools import product as prod\n",
    "\n",
    "models = [xgb_tuned, lgbm_tuned, lr_tuned, rf_tuned]\n",
    "bin = [[0,1]] * len(models)\n",
    "i=0\n",
    "for a0,a1,a2,a3 in prod([0,1],[0,1],[0,1],[0,1]):\n",
    "    if a0+a1+a2+a3==0:\n",
    "        continue\n",
    "    pred_models = []\n",
    "    if a0 == 1:\n",
    "        pred_models.append(models[0])\n",
    "    if a1 == 1:\n",
    "        pred_models.append(models[1])\n",
    "    if a2 == 1:\n",
    "        pred_models.append(models[2])\n",
    "    if a3 == 1:\n",
    "        pred_models.append(models[3])\n",
    "\n",
    "    for i,model in enumerate(pred_models):\n",
    "        if i==0:\n",
    "            all_test_proba = model.predict_proba(X_test)\n",
    "        else:\n",
    "            all_test_proba += model.predict_proba(X_test)\n",
    "    all_test_proba = all_test_proba/(i+1)\n",
    "    best_score, best_thres = get_best_combination(all_test_proba[:,1], y_test)\n",
    "\n",
    "    print(a0,a1,a2,a3, best_score, best_thres)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 통째로 다시 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_under = RandomUnderSampler(sampling_strategy=0.2, random_state=777)\n",
    "X_under_all, y_under_all = random_under.fit_resample(X_scaled, y)\n",
    "\n",
    "smote = SMOTE(random_state=777) # SMOTE의 하이퍼파라미터는 default로 사용\n",
    "X_resampled_all, y_resampled_all=smote.fit_resample(X_under_all, y_under_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost done\n",
      "lgbm done\n",
      "random forest done\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb_tuned_all = xgb.XGBClassifier(random_state=777, n_estimators=100, \n",
    "                            max_depth=20, subsample=1.0, gamma=0, \n",
    "                            tree_method='gpu_hist', gpu_id=0)\n",
    "xgb_tuned_all.fit(X_under_all, y_under_all)\n",
    "print('xgboost done')\n",
    "\n",
    "# LGBM\n",
    "lgbm_tuned_all = LGBMClassifier(num_leaves=70, max_depth=25, min_child_samples=100, \n",
    "                        random_state=777, n_jobs=-1, n_estimators=200, objective='binary', \n",
    "                        is_unbalance=True)\n",
    "lgbm_tuned_all.fit(X_resampled_all, y_resampled_all)\n",
    "print('lgbm done')\n",
    "\n",
    "# Random Forest\n",
    "rf_tuned_all = RandomForestClassifier(n_estimators=100, random_state=777)\n",
    "rf_tuned_all.fit(X_under_all, y_under_all)\n",
    "print('random forest done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed/xgb.pickle','wb') as fw:\n",
    "    pickle.dump(xgb_tuned_all, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed/lgbm.pickle','wb') as fw:\n",
    "    pickle.dump(lgbm_tuned_all, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../preprocessed/random_forest.pickle','wb') as fw:\n",
    "    pickle.dump(rf_tuned_all, fw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ryuvenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db4a054fa3485500d815f3fdd1ed0c3e4f2427f3103f3684095f07e4b5b8fabd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
